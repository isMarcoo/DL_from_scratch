{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    " \n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data  import Field,BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建分词器器\n",
    "spacy_en=spacy.load(\"en_core_web_sm\")#英语分词器\n",
    "spacy_de=spacy.load(\"de_core_news_sm\")#德语分词器\n",
    " \n",
    "def en_seq(text):\n",
    "    return [word.text for word in spacy_en.tokenizer(text)]\n",
    " \n",
    "def de_seq(text):\n",
    "    return [word.text for word in spacy_de.tokenizer(text)][::-1]#源端倒序\n",
    " \n",
    "#源端的处理手段\n",
    "SRC=Field(tokenize=de_seq,\n",
    "         init_token=\"<sos>\",\n",
    "         eos_token=\"<eos>\",\n",
    "         lower=True)\n",
    " \n",
    "#目标端的处理手段\n",
    "TRG=Field(tokenize=en_seq,\n",
    "         init_token=\"<sos>\",\n",
    "         eos_token=\"<eos>\",\n",
    "         lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 29000\n",
      "Number of validation examples: 1014\n",
      "Number of testing examples: 1000\n"
     ]
    }
   ],
   "source": [
    "#定义dataset数据集，这里将其数据经过fiels处理\n",
    "train_data,valid_data,test_data=Multi30k.splits(exts=(\".de\",\".en\"),\n",
    "                                               fields=(SRC,TRG))\n",
    " \n",
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data,min_freq=2)\n",
    "TRG.build_vocab(train_data,min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "cuda=torch.cuda.is_available()\n",
    " \n",
    "train_iterator, valid_iterator, test_iterator=BucketIterator.splits(\n",
    "    (train_data,valid_data,test_data),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    device=torch.device('cuda' if cuda else 'cpu')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, method=\"dot\"):\n",
    "        super(LuongAttention, self).__init__()\n",
    "        self.method = method\n",
    "        if method == \"general\":\n",
    "            self.W = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "    def forward(self, h, encoder_outputs):\n",
    "        # h: [batch_size, hidden_size]\n",
    "        # encoder_outputs: [batch_size, seq_len, hidden_size]\n",
    "        if self.method == \"dot\":\n",
    "            score = torch.bmm(encoder_outputs, h.unsqueeze(2))  # [batch_size, seq_len, 1]\n",
    "        elif self.method == \"general\":\n",
    "            score = self.W(encoder_outputs)  # [batch_size, seq_len, hidden_size]\n",
    "            score = torch.bmm(score, h.unsqueeze(2))  # [batch_size, seq_len, 1]\n",
    "        \n",
    "        attention_weights = torch.softmax(score, dim=1)\n",
    "        context_vector = torch.bmm(encoder_outputs.transpose(1, 2), attention_weights).squeeze(2)\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.V = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, h, encoder_outputs):\n",
    "        # h: [batch_size, hidden_size], decoder 的当前隐藏状态\n",
    "        # encoder_outputs: [batch_size, seq_len, hidden_size], 编码器的所有输出\n",
    "        h = h.unsqueeze(1)  # [batch_size, 1, hidden_size]\n",
    "        score = self.V(torch.tanh(self.W1(encoder_outputs) + self.W2(h)))  # [batch_size, seq_len, 1]\n",
    "        attention_weights = torch.softmax(score, dim=1)  # [batch_size, seq_len, 1]\n",
    "        \n",
    "        context_vector = attention_weights * encoder_outputs  # [batch_size, seq_len, hidden_size]\n",
    "        context_vector = context_vector.sum(1)  # [batch_size, hidden_size]\n",
    "        \n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class  Encoder(nn.Module):\n",
    "    #src_vocab_size德语词汇表大小，emb_model词向量维度，hidden_size隐藏向量维度，n_layers lstm深度\n",
    "    def __init__(self,src_vocab_size,emb_model,hidden_size,n_layers,dropout):\n",
    "        super(Encoder,self).__init__()\n",
    "        \n",
    "        self.embed=nn.Embedding(src_vocab_size,emb_model,padding_idx=1)\n",
    "        self.lstm=nn.LSTM(input_size=emb_model,hidden_size=hidden_size,num_layers=n_layers,batch_first=True,dropout=dropout)\n",
    "    \n",
    "    def forward(self,src):\n",
    "        #src[batch_size,seq_len]\n",
    "        src=self.embed(src)\n",
    "        #src[batch_size,seq_len,emb_model]\n",
    "        output,(h_n,c_n)=self.lstm(src)\n",
    "        #output[batch_size,seq_len,hidden_size]  最后一层每个时间步的隐状态h_t\n",
    "        \n",
    "        #h_n[batch_size,n_layers,hidden_size] 最后一个时间步每层的隐状态(实际上并非这样，Pytorch机制原因)\n",
    "        #c_n[batch_size,n_layers,hidden_size] 最后一个时间步每层的记忆c（实际上并非这样，Pytorch机制原因）\n",
    "        \n",
    "        return output,(h_n,c_n)#output的意义不大，主要是(h_n,c_n)，其作为上下文向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试，参数\n",
    "emb_model=256\n",
    "hidden_size=512\n",
    "n_layers=4\n",
    "dropout=0.5\n",
    "src_vocab_size=len(SRC.vocab)\n",
    "\n",
    "enModel=Encoder(src_vocab_size,emb_model,hidden_size,n_layers,dropout)\n",
    "if(cuda):\n",
    "    enModel=enModel.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    #trg_vocab_size 目标端的词汇表大小\n",
    "    #emb_dim为词向量维度（我们将其设置与源端一样大小）\n",
    "    #hidden_size 为目标端隐层维度（将其设置为与源端一样大小）\n",
    "    #n_layers 网络层数（将其设置为一样大小）\n",
    "    def __init__(self,trg_vocab_size,emb_dim,hidden_size,n_layers,dropout,attention):\n",
    "        super(Decoder,self).__init__()\n",
    "        \n",
    "        self.emb=nn.Embedding(trg_vocab_size,emb_dim,padding_idx=1)\n",
    "        self.lstm=nn.LSTM(emb_dim*3,hidden_size,num_layers=n_layers,batch_first=True,dropout=dropout)\n",
    "        self.classify=nn.Linear(hidden_size,trg_vocab_size)\n",
    "        self.attention = attention\n",
    "    \n",
    "    def forward(self,trg,h_n,c_n,encoder_outputs):\n",
    "        #trg为应该为[batch,seq_len]，不过实际训练中是一个一个传入（要考虑是否采用强制教学），所以seq_len为1\n",
    "        #trg真正的输入维度为[batch]\n",
    "        #h_n与c_n是源端的上下文向量（若计算不指定，则默认为0（若Encoder编码中））\n",
    "        #维度均为：[n_layers,batch_size,hidden_size]\n",
    "        trg=trg.unsqueeze(1)\n",
    "        #trg[batch,1]\n",
    "        embedded=self.emb(trg)\n",
    "        #trg[batch,1,emb]\n",
    "\n",
    "        # 计算注意力权重和上下文向量\n",
    "        context_vector, attention_weights = self.attention(h_n[-1], encoder_outputs)\n",
    "        # 将嵌入层输出和上下文向量结合起来\n",
    "        lstm_input = torch.cat((embedded, context_vector.unsqueeze(1)), dim=2)\n",
    "\n",
    "\n",
    "        output,(h_n,c_n)=self.lstm(lstm_input,(h_n,c_n))#这里的lstm指定了h，c，因此其内部不会自己创建一个全为0的h，c\n",
    "        #output[batch,1,emb]\n",
    "        #h_i[1，batch,emb]\n",
    "        #c_i[1,batch,emb]\n",
    "        output=self.classify(output.squeeze())#output.squeeze()使得output[batch 1 emb]->[batch emb]\n",
    "        #output[batch trg_vocab_size]\n",
    "        return output,(h_n,c_n) #返回(h_n,c_n)是为了下一解码器继续使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试，参数\n",
    "trg_vocab_size=len(TRG.vocab)\n",
    "\n",
    "attention = LuongAttention(hidden_size)\n",
    "\n",
    "Demodel=Decoder(trg_vocab_size,emb_model,hidden_size,n_layers,dropout, attention)\n",
    "if cuda:\n",
    "    Demodel=Demodel.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,encoder,decoder):\n",
    "        super(Seq2Seq,self).__init__()\n",
    "        self.encoder=encoder\n",
    "        self.decoder=decoder\n",
    "    \n",
    "    def forward(self,src,trg,teach_rate=0.5):\n",
    "        #src [bacth,seq_len]\n",
    "        #trg  [bacth,seq_len]\n",
    "        #teach_radio 强制教学的阈值\n",
    "        batch_size=trg.shape[0]\n",
    "        trg_seqlen=trg.shape[1]\n",
    "        \n",
    "        #保存每次输出的结果\n",
    "        outputs_save=torch.zeros(batch_size,trg_seqlen,trg_vocab_size)\n",
    "        if(cuda):\n",
    "            outputs_save=outputs_save.cuda()\n",
    "        #对源端进行编码\n",
    "        encoder_outputs,(h_n,c_n)=self.encoder(src)\n",
    "        \n",
    "        #第一个输入到解码器中为<sos>\n",
    "        trg_i=trg[:,0]\n",
    "        #trg_i [batch]\n",
    "        for i in range(1,trg_seqlen):\n",
    "            output,(h_n,c_n)=self.decoder(trg_i,h_n,c_n,encoder_outputs)\n",
    "            #output[batch trg_vocab_size]\n",
    "            outputs_save[:,i,:]=output\n",
    "            #产生一个随机概率(即是否强制教学)\n",
    "            probability=random.random()\n",
    "            \n",
    "            #获取时间步预测的结果\n",
    "            top=output.argmax(1)\n",
    "            #top[batch]\n",
    "            #下一时间步的输入\n",
    "            trg_i=trg[:,i] if probability>teach_rate else top\n",
    "        return outputs_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Seq2Seq(enModel,Demodel)\n",
    "if(cuda):\n",
    "    model=model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "epochs=10\n",
    "optim=Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = 1)#pad不参与损失函数的计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embed): Embedding(7853, 256, padding_idx=1)\n",
       "    (lstm): LSTM(256, 512, num_layers=4, batch_first=True, dropout=0.5)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (emb): Embedding(5893, 256, padding_idx=1)\n",
       "    (lstm): LSTM(768, 512, num_layers=4, batch_first=True, dropout=0.5)\n",
       "    (classify): Linear(in_features=512, out_features=5893, bias=True)\n",
       "    (attention): LuongAttention()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#参数初始化\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,train_iter,optim,criterion):\n",
    "    model.train()#即dropout产生作用\n",
    "    epoch_loss=0\n",
    "    for i,example in enumerate(train_iter):\n",
    "        src=example.src.permute(1,0)\n",
    "        trg=example.trg.permute(1,0)\n",
    "        #src[batch seqlen]\n",
    "        #trg[batch seqlen]\n",
    " \n",
    "        optim.zero_grad()\n",
    "        output=model(src,trg)#output[batch trg_seqlen trq_vocab_size]\n",
    "        #<sos>不参与运算，pad也不参与运算（criterion已经设置了ignore）\n",
    "        output=output[:,1:,:].reshape(-1,trg_vocab_size)\n",
    "        trg=trg[:,1:].reshape(-1)\n",
    "        #output[batch*(trg_len-1),trg_vocab_size]\n",
    "        #trg[batch*(trg_len-1)]\n",
    "        loss=criterion(output,trg)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        epoch_loss+=loss.item()\n",
    "    return epoch_loss/len(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model,test_iter,criterion):\n",
    "    model.eval()#即dropout产生作用\n",
    "    epoch_loss=0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i,example in enumerate(test_iter):\n",
    "            \n",
    "            src=example.src.permute(1,0)\n",
    "            trg=example.trg.permute(1,0)\n",
    "            #src[batch seqlen]\n",
    "            #trg[batch seqlen]\n",
    " \n",
    " \n",
    "            #即无法在进行强制教学\n",
    "            output=model(src,trg,0)#output[batch trg_seqlen trq_vocab_size]\n",
    "            \n",
    "            #<sos>不参与运算，pad也不参与运算（criterion已经设置了ignore）\n",
    "            output=output[:,1:].reshape(-1,trg_vocab_size)\n",
    "            trg=trg[:,1:].reshape(-1)\n",
    "            #output[batch*(trg_len-1),trg_vocab_size]\n",
    "            #trg[batch*(trg_len-1)]\n",
    "            loss=criterion(output,trg)\n",
    "            epoch_loss+=loss.item()\n",
    " \n",
    "    return epoch_loss/len(test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time,math\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LuongAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 33s\n",
      "\tTrain Loss: 5.150 | Train PPL: 172.405\n",
      "\t Val. Loss: 4.874 |  Val. PPL: 130.809\n",
      "Epoch: 02 | Time: 0m 34s\n",
      "\tTrain Loss: 4.884 | Train PPL: 132.124\n",
      "\t Val. Loss: 4.295 |  Val. PPL:  73.357\n",
      "Epoch: 03 | Time: 0m 34s\n",
      "\tTrain Loss: 4.637 | Train PPL: 103.183\n",
      "\t Val. Loss: 4.059 |  Val. PPL:  57.926\n",
      "Epoch: 04 | Time: 0m 35s\n",
      "\tTrain Loss: 4.535 | Train PPL:  93.189\n",
      "\t Val. Loss: 4.006 |  Val. PPL:  54.935\n",
      "Epoch: 05 | Time: 0m 38s\n",
      "\tTrain Loss: 4.439 | Train PPL:  84.725\n",
      "\t Val. Loss: 3.920 |  Val. PPL:  50.423\n",
      "Epoch: 06 | Time: 0m 38s\n",
      "\tTrain Loss: 4.151 | Train PPL:  63.473\n",
      "\t Val. Loss: 3.575 |  Val. PPL:  35.694\n",
      "Epoch: 07 | Time: 0m 35s\n",
      "\tTrain Loss: 3.924 | Train PPL:  50.624\n",
      "\t Val. Loss: 3.355 |  Val. PPL:  28.653\n",
      "Epoch: 08 | Time: 0m 35s\n",
      "\tTrain Loss: 3.719 | Train PPL:  41.240\n",
      "\t Val. Loss: 3.177 |  Val. PPL:  23.981\n",
      "Epoch: 09 | Time: 0m 35s\n",
      "\tTrain Loss: 3.541 | Train PPL:  34.502\n",
      "\t Val. Loss: 3.044 |  Val. PPL:  20.999\n",
      "Epoch: 10 | Time: 0m 35s\n",
      "\tTrain Loss: 3.389 | Train PPL:  29.649\n",
      "\t Val. Loss: 2.922 |  Val. PPL:  18.569\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    start_time=time.time()\n",
    "    \n",
    "    train_loss=train(model,train_iterator,optim,criterion)\n",
    "    valid_loss=evaluate(model,valid_iterator,criterion)\n",
    "    \n",
    "    end_time=time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BahdanauAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 42s\n",
      "\tTrain Loss: 5.144 | Train PPL: 171.460\n",
      "\t Val. Loss: 4.881 |  Val. PPL: 131.774\n",
      "Epoch: 02 | Time: 0m 42s\n",
      "\tTrain Loss: 4.834 | Train PPL: 125.772\n",
      "\t Val. Loss: 4.297 |  Val. PPL:  73.499\n",
      "Epoch: 03 | Time: 0m 43s\n",
      "\tTrain Loss: 4.437 | Train PPL:  84.535\n",
      "\t Val. Loss: 3.865 |  Val. PPL:  47.704\n",
      "Epoch: 04 | Time: 0m 43s\n",
      "\tTrain Loss: 4.200 | Train PPL:  66.672\n",
      "\t Val. Loss: 3.655 |  Val. PPL:  38.673\n",
      "Epoch: 05 | Time: 0m 44s\n",
      "\tTrain Loss: 4.036 | Train PPL:  56.574\n",
      "\t Val. Loss: 3.532 |  Val. PPL:  34.182\n",
      "Epoch: 06 | Time: 0m 44s\n",
      "\tTrain Loss: 3.922 | Train PPL:  50.493\n",
      "\t Val. Loss: 3.429 |  Val. PPL:  30.843\n",
      "Epoch: 07 | Time: 0m 45s\n",
      "\tTrain Loss: 3.811 | Train PPL:  45.182\n",
      "\t Val. Loss: 3.331 |  Val. PPL:  27.970\n",
      "Epoch: 08 | Time: 0m 44s\n",
      "\tTrain Loss: 3.706 | Train PPL:  40.675\n",
      "\t Val. Loss: 3.251 |  Val. PPL:  25.815\n",
      "Epoch: 09 | Time: 0m 44s\n",
      "\tTrain Loss: 3.607 | Train PPL:  36.860\n",
      "\t Val. Loss: 3.178 |  Val. PPL:  23.995\n",
      "Epoch: 10 | Time: 0m 43s\n",
      "\tTrain Loss: 3.522 | Train PPL:  33.857\n",
      "\t Val. Loss: 3.123 |  Val. PPL:  22.724\n"
     ]
    }
   ],
   "source": [
    "attention = BahdanauAttention(hidden_size)\n",
    "\n",
    "Demodel=Decoder(trg_vocab_size,emb_model,hidden_size,n_layers,dropout, attention)\n",
    "if cuda:\n",
    "    Demodel=Demodel.cuda()\n",
    "\n",
    "model=Seq2Seq(enModel,Demodel)\n",
    "if(cuda):\n",
    "    model=model.cuda()\n",
    "\n",
    "epochs=10\n",
    "optim=Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = 1)#pad不参与损失函数的计算\n",
    "\n",
    "#参数初始化\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time=time.time()\n",
    "    \n",
    "    train_loss=train(model,train_iterator,optim,criterion)\n",
    "    valid_loss=evaluate(model,valid_iterator,criterion)\n",
    "    \n",
    "    end_time=time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model, src_sentence, SRC, TRG, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Tokenize sentence if not already.\n",
    "        if isinstance(src_sentence, str):\n",
    "            spacy_de = spacy.load(\"de_core_news_sm\")\n",
    "            tokens = [tok.text.lower() for tok in spacy_de(src_sentence)]\n",
    "        else:\n",
    "            tokens = [tok.lower() for tok in src_sentence]\n",
    "\n",
    "        # Add <sos> and <eos> tokens\n",
    "        tokens = [SRC.init_token] + tokens + [TRG.eos_token]\n",
    "        \n",
    "        # Convert to indices\n",
    "        src_indexes = [SRC.vocab.stoi[token] for token in tokens]\n",
    "        src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Forward pass through the encoder\n",
    "        encoder_outputs, hidden = model.encoder(src_tensor)\n",
    "        \n",
    "        # First input to the decoder is the <sos> token\n",
    "        trg_indexes = [TRG.vocab.stoi[TRG.init_token]]\n",
    "        \n",
    "        # Initial context is the last hidden state from the encoder\n",
    "        context=hidden.permute(1,0,2)\n",
    "\n",
    "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
    "        input=trg_tensor[:,0]\n",
    "\n",
    "        # Begin decoding\n",
    "        for i in range(100):  # assuming maximum length of the translated sentence is 100 tokens\n",
    "            \n",
    "            \n",
    "            # Compute the attention and output from decoder\n",
    "            output, hidden = model.decoder(input, context, hidden, encoder_outputs)\n",
    "            \n",
    "            # Select the token with the highest probability\n",
    "            pred_token = output.argmax(1).item()\n",
    "            trg_indexes.append(pred_token)\n",
    "\n",
    "            # If <eos> token is generated, stop\n",
    "            if pred_token == TRG.vocab.stoi[TRG.eos_token]:\n",
    "                break\n",
    "\n",
    "        # Convert indices to words\n",
    "        trg_tokens = [TRG.vocab.itos[i] for i in trg_indexes]\n",
    "\n",
    "        # Return the generated tokens, removing the initial <sos>\n",
    "        return trg_tokens[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score = 37.75\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "def calculate_bleu(data, src_field, trg_field, model, device, max_len = 50):\n",
    "\n",
    "    trgs = []\n",
    "    pred_trgs = []\n",
    "\n",
    "    for datum in data:\n",
    "\n",
    "        src = vars(datum)['src']\n",
    "        trg = vars(datum)['trg']\n",
    "\n",
    "        pred_trg, _ = translate(model, src, src_field, trg_field, device)\n",
    "\n",
    "        #cut off <eos> token\n",
    "        pred_trg = pred_trg[:-1]\n",
    "\n",
    "        pred_trgs.append(pred_trg)\n",
    "        trgs.append([trg])\n",
    "\n",
    "    return bleu_score(pred_trgs, trgs)\n",
    "bleu_score = calculate_bleu(test_data, SRC, TRG, model, 'cuda')\n",
    "print(f\"BLEU score: {bleu_score:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
