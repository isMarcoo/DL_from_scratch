{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import spacy\n",
    " \n",
    "from torchtext.data import Field,BucketIterator\n",
    "from torchtext.datasets import Multi30k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda=torch.cuda.is_available()\n",
    "device=torch.device(\"cuda\" if cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_de=spacy.load(\"de_core_news_sm\")\n",
    "spacy_en=spacy.load(\"en_core_web_sm\")\n",
    " \n",
    "def de_seq(text):\n",
    "    #return [word.text for word in spacy_de.tokenizer(text)][::-1]这里并未进行倒序\n",
    "    return [word.text for word in spacy_de.tokenizer(text)]\n",
    " \n",
    "def en_seq(text):\n",
    "    return [word.text for word in spacy_en.tokenizer(text)]\n",
    " \n",
    "SRC=Field(tokenize=de_seq,\n",
    "         init_token=\"<sos>\",\n",
    "         eos_token=\"<eos>\",\n",
    "         lower=True)\n",
    "TRG=Field(tokenize=en_seq,\n",
    "         init_token=\"<sos>\",\n",
    "         eos_token=\"<eos>\",\n",
    "         lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,val_data,test_data=Multi30k.splits(exts=(\".de\",\".en\"),\n",
    "                                             fields=(SRC,TRG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data,min_freq=2)\n",
    "TRG.build_vocab(train_data,min_freq=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128\n",
    " \n",
    "train_iter,val_iter,test_iter=BucketIterator.splits(\n",
    "    (train_data,val_data,test_data),\n",
    "    batch_size=batch_size,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, method=\"dot\"):\n",
    "        super(LuongAttention, self).__init__()\n",
    "        self.method = method\n",
    "        if method == \"general\":\n",
    "            self.W = nn.Linear(hidden_size, hidden_size)\n",
    "        \n",
    "    def forward(self, h, encoder_outputs):\n",
    "        # h: [batch_size, hidden_size]\n",
    "        # encoder_outputs: [batch_size, seq_len, hidden_size]\n",
    "        if self.method == \"dot\":\n",
    "            score = torch.bmm(encoder_outputs, h.unsqueeze(2))  # [batch_size, seq_len, 1]\n",
    "        elif self.method == \"general\":\n",
    "            score = self.W(encoder_outputs)  # [batch_size, seq_len, hidden_size]\n",
    "            score = torch.bmm(score, h.unsqueeze(2))  # [batch_size, seq_len, 1]\n",
    "        \n",
    "        attention_weights = torch.softmax(score, dim=1)\n",
    "        context_vector = torch.bmm(encoder_outputs.transpose(1, 2), attention_weights).squeeze(2)\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.V = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, h, encoder_outputs):\n",
    "        # h: [batch_size, hidden_size], decoder 的当前隐藏状态\n",
    "        # encoder_outputs: [batch_size, seq_len, hidden_size], 编码器的所有输出\n",
    "        h = h.unsqueeze(1)  # [batch_size, 1, hidden_size]\n",
    "        score = self.V(torch.tanh(self.W1(encoder_outputs) + self.W2(h)))  # [batch_size, seq_len, 1]\n",
    "        attention_weights = torch.softmax(score, dim=1)  # [batch_size, seq_len, 1]\n",
    "        \n",
    "        context_vector = attention_weights * encoder_outputs  # [batch_size, seq_len, hidden_size]\n",
    "        context_vector = context_vector.sum(1)  # [batch_size, hidden_size]\n",
    "        \n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,src_vocab_size,embed_size,hidden_size,dropout=0.5):\n",
    "        #src_vocab_size 为德语词库的大小 enbed_size,词向量大小\n",
    "        #hidden_size 隐藏状态的维度\n",
    "        \n",
    "        super(Encoder,self).__init__()\n",
    "        self.embedding=nn.Embedding(src_vocab_size,embed_size,padding_idx=1)\n",
    "        self.rnn=nn.GRU(embed_size,hidden_size,batch_first=True)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self,src):\n",
    "        #src [batch seq_len]\n",
    "        x_embeding=self.dropout(self.embedding(src))\n",
    "        #x_embeding [batch seq_len embed_size]\n",
    "        encoder_outputs,h_n=self.rnn(x_embeding)\n",
    "        #h_n也是我们要的上下文向量，其为最后一个时间步各层的输出，即[n_layers,batch_size,hidden_size]\n",
    "        #这里就是[1,batch_size,hidden_size]\n",
    "        return encoder_outputs, h_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试，也是最终模型的参数\n",
    "src_vocab_size=len(SRC.vocab)\n",
    "trg_vocab_size=len(TRG.vocab)\n",
    "embed_size=256\n",
    "hidden_size=512\n",
    "\n",
    "enModel=Encoder(src_vocab_size,embed_size,hidden_size)\n",
    "if cuda:\n",
    "    enModel=enModel.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,trg_vocab_size,embed_size,hidden_size,dropout=0.5,attention=None):\n",
    "        super(Decoder,self).__init__()\n",
    "        #trg_vocab_size为英语词库大小 embed_size词向量维度\n",
    "        #hidden_size隐状态大小\n",
    "        self.embedding=nn.Embedding(trg_vocab_size,embed_size,padding_idx=1)\n",
    "        #输入多了原始上下文向量，这里我们将源端和目标端的隐状态维度都默认为hidden_size\n",
    "        self.rnn=nn.GRU(embed_size+hidden_size,hidden_size,batch_first=True)\n",
    "        self.classify=nn.Linear(embed_size+hidden_size*2,trg_vocab_size)\n",
    "        self.dropout=nn.Dropout(dropout)\n",
    "        self.attention=attention\n",
    "        \n",
    "    def forward(self,trg_i,context,h_n,encoder_outputs):\n",
    "        #trg_i为某一时间步词的输入，[bacth_size]\n",
    "        #context为原始上下文向量，[bacth_size,1,hidden_size]\n",
    "        #h_n为上一时间布的隐状态[1，batch_size，hidden_size]\n",
    "        trg_i=trg_i.unsqueeze(1)\n",
    "        #trg_i[bacth_size,1]\n",
    "        trg_i_embed=self.dropout(self.embedding(trg_i))\n",
    "        #trg_i_embed [bacth_size,1,embed_size]\n",
    "        \n",
    "        context_vector, _ = self.attention(h_n[-1], encoder_outputs)\n",
    "        rnn_input = torch.cat((trg_i_embed, context_vector.unsqueeze(1)), dim=2)\n",
    "\n",
    "\n",
    "        #输入rnn模块的不仅仅只有词嵌入和上一时间步的隐状态，还有原始上下向量\n",
    "        input=torch.cat((trg_i_embed,context),dim=2)\n",
    "        #input[bacth_size,1,embed_size+hidden_size]\n",
    "        output,h_n=self.rnn(rnn_input,h_n)\n",
    "        #output[batch_size,1,hidden_size]\n",
    "        #h_n[1,batch_size,hidden_size]\n",
    "        \n",
    "        \n",
    "        #原本rnn模型的输入直接带入线性分类层映射到英语空间中，这里新添原始词嵌入和原始上下文向量，即上面的input\n",
    "        input=rnn_input.squeeze()\n",
    "        output=output.squeeze()\n",
    "        #input[bacth_size embed_size+hidden_size]\n",
    "        #output[batch_szie hidden_size]\n",
    "        input=torch.cat((input,output),dim=1)\n",
    "        output=self.classify(input)\n",
    "        #output[bacth trg_vocab_size]\n",
    "        return output,h_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = LuongAttention(hidden_size)\n",
    "\n",
    "deModel=Decoder(trg_vocab_size,embed_size,hidden_size,attention=attention)\n",
    "if cuda:\n",
    "    deModel=deModel.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#这一步其实就整合上面我们的测试\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,encoder,decoder):\n",
    "        super(Seq2Seq,self).__init__()\n",
    "        self.encoder=encoder\n",
    "        self.decoder=decoder\n",
    "        \n",
    "    def forward(self,src,trg,teach_threshold=0.5):\n",
    "        #src[batch seq_len]\n",
    "        #trg[bacth seq_len]\n",
    "        #teacher_rate 进行forcing teaching 的阈值\n",
    "        trg_seq_len=trg.shape[1]\n",
    "        #这一步很重要，最后一个batch大小没有batch_size大小\n",
    "        batch=trg.shape[0]\n",
    "        \n",
    "        #设置一个tensor保存所有预测结果\n",
    "        outputs_save=torch.zeros(batch,trg_seq_len,trg_vocab_size)\n",
    "        if cuda:\n",
    "            outputs_save=outputs_save.cuda()\n",
    "        \n",
    "        #获取编码层的输出\n",
    "        encoder_outputs, h_n=self.encoder(src)\n",
    "        #h_n[1,batch_size,hidden_size]\n",
    "        context=h_n.permute(1,0,2)\n",
    "        #context[batch_size,1,hidden_size]\n",
    "        input=trg[:,0]\n",
    "        \n",
    "        #遍历每个英语的输入，代入翻译\n",
    "        for t in range(1,trg_seq_len):\n",
    "            #将每个时间步词典代入计算\n",
    "            output,h_n=self.decoder(input,context,h_n,encoder_outputs)\n",
    "            #保存rnn模型经过线性分类层的输出\n",
    "            outputs_save[:,t,:]=output\n",
    "            probability=random.random()\n",
    "            #是否采用强制教学\n",
    "            input=trg[:,t] if probability<teach_threshold else output.argmax(1)\n",
    "        return outputs_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Seq2Seq(enModel,deModel)\n",
    "if cuda:\n",
    "    model=model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time,math\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=10\n",
    "criterion=nn.CrossEntropyLoss(ignore_index=1)\n",
    "model = Seq2Seq(enModel, deModel)\n",
    "optim=Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(7853, 256, padding_idx=1)\n",
       "    (rnn): GRU(256, 512, batch_first=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(5893, 256, padding_idx=1)\n",
       "    (rnn): GRU(768, 512, batch_first=True)\n",
       "    (classify): Linear(in_features=1280, out_features=5893, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "    (attention): LuongAttention()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,train_iter,criterion,optim):\n",
    "    model.train()\n",
    "    lossAll=0\n",
    "    for example in train_iter:\n",
    "        src=example.src.permute(1,0)\n",
    "        trg=example.trg.permute(1,0)\n",
    "        #src[batch seq_len]\n",
    "        #trg[batch seq_len]\n",
    "        \n",
    "        optim.zero_grad()\n",
    "        output=model(src,trg)\n",
    "        #output[batch seq_len trg_vocab_size]\n",
    "        trg_vocab_size=output.shape[-1]\n",
    "        output=output[:,1:,:].reshape(-1,trg_vocab_size)\n",
    "        #output[bacth*(seq_len-1),trg_vocab_size]\n",
    "        trg=trg[:,1:].reshape(-1)\n",
    "        #trg[bacth*(seq_len-1)]\n",
    "        \n",
    "        loss=criterion(output,trg)\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        lossAll+=loss.item()\n",
    "    return lossAll/len(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model,val_iter,criterion):\n",
    "    model.eval()\n",
    "    lossAll=0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for example in val_iter:\n",
    "            src=example.src.permute(1,0)\n",
    "            trg=example.trg.permute(1,0)\n",
    "            #src[batch seq_len]\n",
    "            #trg[batch seq_len]\n",
    "            \n",
    "            output=model(src,trg)\n",
    "            #output[batch seq_len trg_vocab_size]\n",
    "            trg_vocab_size=output.shape[-1]\n",
    "            output=output[:,1:,:].reshape(-1,trg_vocab_size)\n",
    "            #output[bacth*(seq_len-1),trg_vocab_size]\n",
    "            trg=trg[:,1:].reshape(-1)\n",
    "            #trg[bacth*(seq_len-1)]\n",
    "            loss=criterion(output,trg)\n",
    "            lossAll+=loss.item()\n",
    " \n",
    "    return lossAll/len(val_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LuongAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 33s\n",
      "\tTrain Loss: 1.597 | Train PPL:   4.937\n",
      "\t Val. Loss: 2.380 |  Val. PPL:  10.803\n",
      "Epoch: 02 | Time: 0m 33s\n",
      "\tTrain Loss: 1.500 | Train PPL:   4.481\n",
      "\t Val. Loss: 2.307 |  Val. PPL:  10.046\n",
      "Epoch: 03 | Time: 0m 33s\n",
      "\tTrain Loss: 1.399 | Train PPL:   4.050\n",
      "\t Val. Loss: 2.438 |  Val. PPL:  11.449\n",
      "Epoch: 04 | Time: 0m 34s\n",
      "\tTrain Loss: 1.336 | Train PPL:   3.805\n",
      "\t Val. Loss: 2.449 |  Val. PPL:  11.581\n",
      "Epoch: 05 | Time: 0m 35s\n",
      "\tTrain Loss: 1.261 | Train PPL:   3.529\n",
      "\t Val. Loss: 2.445 |  Val. PPL:  11.531\n",
      "Epoch: 06 | Time: 0m 35s\n",
      "\tTrain Loss: 1.195 | Train PPL:   3.304\n",
      "\t Val. Loss: 2.516 |  Val. PPL:  12.379\n",
      "Epoch: 07 | Time: 0m 35s\n",
      "\tTrain Loss: 1.095 | Train PPL:   2.990\n",
      "\t Val. Loss: 2.695 |  Val. PPL:  14.813\n",
      "Epoch: 08 | Time: 0m 35s\n",
      "\tTrain Loss: 1.057 | Train PPL:   2.878\n",
      "\t Val. Loss: 2.505 |  Val. PPL:  12.246\n",
      "Epoch: 09 | Time: 0m 36s\n",
      "\tTrain Loss: 1.033 | Train PPL:   2.810\n",
      "\t Val. Loss: 2.633 |  Val. PPL:  13.909\n",
      "Epoch: 10 | Time: 0m 36s\n",
      "\tTrain Loss: 0.978 | Train PPL:   2.660\n",
      "\t Val. Loss: 2.660 |  Val. PPL:  14.303\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss = train(model,train_iter,criterion,optim)\n",
    "    valid_loss = evaluate(model,val_iter,criterion)\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BahdanauAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 0m 35s\n",
      "\tTrain Loss: 5.047 | Train PPL: 155.493\n",
      "\t Val. Loss: 4.466 |  Val. PPL:  87.014\n",
      "Epoch: 02 | Time: 0m 36s\n",
      "\tTrain Loss: 4.377 | Train PPL:  79.565\n",
      "\t Val. Loss: 4.079 |  Val. PPL:  59.076\n",
      "Epoch: 03 | Time: 0m 36s\n",
      "\tTrain Loss: 4.029 | Train PPL:  56.193\n",
      "\t Val. Loss: 3.834 |  Val. PPL:  46.255\n",
      "Epoch: 04 | Time: 0m 36s\n",
      "\tTrain Loss: 3.585 | Train PPL:  36.061\n",
      "\t Val. Loss: 3.180 |  Val. PPL:  24.035\n",
      "Epoch: 05 | Time: 0m 36s\n",
      "\tTrain Loss: 3.096 | Train PPL:  22.118\n",
      "\t Val. Loss: 2.796 |  Val. PPL:  16.376\n",
      "Epoch: 06 | Time: 0m 35s\n",
      "\tTrain Loss: 2.706 | Train PPL:  14.962\n",
      "\t Val. Loss: 2.629 |  Val. PPL:  13.861\n",
      "Epoch: 07 | Time: 0m 36s\n",
      "\tTrain Loss: 2.437 | Train PPL:  11.438\n",
      "\t Val. Loss: 2.516 |  Val. PPL:  12.379\n",
      "Epoch: 08 | Time: 0m 36s\n",
      "\tTrain Loss: 2.204 | Train PPL:   9.064\n",
      "\t Val. Loss: 2.533 |  Val. PPL:  12.586\n",
      "Epoch: 09 | Time: 0m 36s\n",
      "\tTrain Loss: 1.988 | Train PPL:   7.300\n",
      "\t Val. Loss: 2.313 |  Val. PPL:  10.105\n",
      "Epoch: 10 | Time: 0m 36s\n",
      "\tTrain Loss: 1.841 | Train PPL:   6.305\n",
      "\t Val. Loss: 2.277 |  Val. PPL:   9.747\n"
     ]
    }
   ],
   "source": [
    "attention = BahdanauAttention(hidden_size)\n",
    "\n",
    "deModel=Decoder(trg_vocab_size,embed_size,hidden_size,attention=attention)\n",
    "if cuda:\n",
    "    deModel=deModel.cuda()\n",
    "\n",
    "model=Seq2Seq(enModel,deModel)\n",
    "if cuda:\n",
    "    model=model.cuda()\n",
    "\n",
    "epochs=10\n",
    "criterion=nn.CrossEntropyLoss(ignore_index=1)\n",
    "model = Seq2Seq(enModel, deModel)\n",
    "optim=Adam(model.parameters())\n",
    "\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        \n",
    "model.apply(init_weights)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    train_loss = train(model,train_iter,criterion,optim)\n",
    "    valid_loss = evaluate(model,val_iter,criterion)\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(model, src_sentence, SRC, TRG, device=\"cuda\"):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Tokenize sentence if not already.\n",
    "        if isinstance(src_sentence, str):\n",
    "            spacy_de = spacy.load(\"de_core_news_sm\")\n",
    "            tokens = [tok.text.lower() for tok in spacy_de(src_sentence)]\n",
    "        else:\n",
    "            tokens = [tok.lower() for tok in src_sentence]\n",
    "\n",
    "        # Add <sos> and <eos> tokens\n",
    "        tokens = [SRC.init_token] + tokens + [TRG.eos_token]\n",
    "        \n",
    "        # Convert to indices\n",
    "        src_indexes = [SRC.vocab.stoi[token] for token in tokens]\n",
    "        src_tensor = torch.LongTensor(src_indexes).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Forward pass through the encoder\n",
    "        encoder_outputs, hidden = model.encoder(src_tensor)\n",
    "        \n",
    "        # First input to the decoder is the <sos> token\n",
    "        trg_indexes = [TRG.vocab.stoi[TRG.init_token]]\n",
    "        \n",
    "        # Initial context is the last hidden state from the encoder\n",
    "        context=hidden.permute(1,0,2)\n",
    "\n",
    "        trg_tensor = torch.LongTensor(trg_indexes).unsqueeze(0).to(device)\n",
    "        input=trg_tensor[:,0]\n",
    "\n",
    "        # Begin decoding\n",
    "        for i in range(100):  # assuming maximum length of the translated sentence is 100 tokens\n",
    "            \n",
    "            \n",
    "            # Compute the attention and output from decoder\n",
    "            output, hidden = model.decoder(input, context, hidden, encoder_outputs)\n",
    "            \n",
    "            # Select the token with the highest probability\n",
    "            pred_token = output.argmax(1).item()\n",
    "            trg_indexes.append(pred_token)\n",
    "\n",
    "            # If <eos> token is generated, stop\n",
    "            if pred_token == TRG.vocab.stoi[TRG.eos_token]:\n",
    "                break\n",
    "\n",
    "        # Convert indices to words\n",
    "        trg_tokens = [TRG.vocab.itos[i] for i in trg_indexes]\n",
    "\n",
    "        # Return the generated tokens, removing the initial <sos>\n",
    "        return trg_tokens[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score = 35.25\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "def calculate_bleu(data, src_field, trg_field, model, device, max_len = 50):\n",
    "\n",
    "    trgs = []\n",
    "    pred_trgs = []\n",
    "\n",
    "    for datum in data:\n",
    "\n",
    "        src = vars(datum)['src']\n",
    "        trg = vars(datum)['trg']\n",
    "\n",
    "        pred_trg, _ = translate(model, src, src_field, trg_field, device)\n",
    "\n",
    "        #cut off <eos> token\n",
    "        pred_trg = pred_trg[:-1]\n",
    "\n",
    "        pred_trgs.append(pred_trg)\n",
    "        trgs.append([trg])\n",
    "\n",
    "    return bleu_score(pred_trgs, trgs)\n",
    "bleu_score = calculate_bleu(test_data, SRC, TRG, model, device)\n",
    "print(f\"BLEU score: {bleu_score:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
