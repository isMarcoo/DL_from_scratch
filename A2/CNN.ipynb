{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_regularization(weights, lam=0.01):\n",
    "    return lam * np.sign(weights)\n",
    "\n",
    "def l2_regularization(weights, lam=0.01):\n",
    "    return lam * weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer:\n",
    "    def __init__(self, input_channels, num_filters, kernel_size, stride=1, padding=0, learning_rate=0.01):\n",
    "        self.input_channels = input_channels\n",
    "        self.num_filters = num_filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.weights = np.random.randn(num_filters, input_channels, kernel_size, kernel_size) * 0.01\n",
    "        self.bias = np.random.randn(num_filters, 1)\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.last_input = x\n",
    "        batch_size, channels, height, width = x.shape\n",
    "        \n",
    "        # Add padding to the input\n",
    "        padded_height = height + 2 * self.padding\n",
    "        padded_width = width + 2 * self.padding\n",
    "        padded_input = np.zeros((batch_size, channels, padded_height, padded_width))\n",
    "        padded_input[:, :, self.padding:height+self.padding, self.padding:width+self.padding] = x\n",
    "        \n",
    "        out_height = (padded_height - self.kernel_size) // self.stride + 1\n",
    "        out_width = (padded_width - self.kernel_size) // self.stride + 1\n",
    "        output = np.zeros((batch_size, self.num_filters, out_height, out_width))\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            for f in range(self.num_filters):\n",
    "                for y in range(out_height):\n",
    "                    for x in range(out_width):\n",
    "                        output[i, f, y, x] = np.sum(\n",
    "                            padded_input[i, :, y*self.stride:y*self.stride+self.kernel_size, x*self.stride:x*self.stride+self.kernel_size] * self.weights[f]) + self.bias[f][0]\n",
    "                        \n",
    "        return output\n",
    "    \n",
    "    def backward(self, d_out):\n",
    "        batch_size, _, out_height, out_width = d_out.shape\n",
    "        d_weights = np.zeros_like(self.weights)\n",
    "        d_bias = np.zeros_like(self.bias)\n",
    "        d_padded_input = np.zeros((batch_size, self.input_channels, self.last_input.shape[2] + 2 * self.padding, self.last_input.shape[3] + 2 * self.padding))\n",
    "        \n",
    "        padded_input = np.zeros((batch_size, self.input_channels, self.last_input.shape[2] + 2 * self.padding, self.last_input.shape[3] + 2 * self.padding))\n",
    "        padded_input[:, :, self.padding:-self.padding, self.padding:-self.padding] = self.last_input\n",
    "\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            for f in range(self.num_filters):\n",
    "                for y in range(out_height):\n",
    "                    for x in range(out_width):\n",
    "                        current_d_out = d_out[i, f, y, x]\n",
    "                        d_weights[f] += current_d_out * padded_input[i, :, y*self.stride:y*self.stride+self.kernel_size, x*self.stride:x*self.stride+self.kernel_size]\n",
    "                        d_bias[f] += current_d_out\n",
    "                        d_padded_input[i, :, y*self.stride:y*self.stride+self.kernel_size, x*self.stride:x*self.stride+self.kernel_size] += current_d_out * self.weights[f]\n",
    "        \n",
    "        # Remove padding from gradient\n",
    "        d_input = d_padded_input[:, :, self.padding:self.last_input.shape[2]+self.padding, self.padding:self.last_input.shape[3]+self.padding]\n",
    "        \n",
    "        self.weights -= self.learning_rate * d_weights\n",
    "        self.bias -= self.learning_rate * d_bias\n",
    "        return d_input, d_weights, d_bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoolLayer:\n",
    "    def __init__(self, pool_size, stride):\n",
    "        self.pool_size = pool_size\n",
    "        self.stride = stride\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.last_input = x\n",
    "        batch_size, channels, height, width = x.shape\n",
    "        out_height = (height - self.pool_size) // self.stride + 1\n",
    "        out_width = (width - self.pool_size) // self.stride + 1\n",
    "        output = np.zeros((batch_size, channels, out_height, out_width))\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            for c in range(channels):\n",
    "                for y in range(0, height - self.pool_size + 1, self.stride):\n",
    "                    for x_ in range(0, width - self.pool_size + 1, self.stride):\n",
    "                        region = x[i, c, y:y+self.pool_size, x_:x_+self.pool_size]\n",
    "                        output[i, c, y // self.stride, x_ // self.stride] = np.max(region)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        d_input = np.zeros_like(self.last_input)\n",
    "        batch_size, channels, out_height, out_width = d_out.shape\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            for c in range(channels):\n",
    "                for y in range(out_height):\n",
    "                    for x in range(out_width):\n",
    "                        current_region = self.last_input[i, c, y*self.stride:y*self.stride+self.pool_size, x*self.stride:x*self.stride+self.pool_size]\n",
    "                        current_max = np.max(current_region)\n",
    "                        for dy in range(self.pool_size):\n",
    "                            for dx in range(self.pool_size):\n",
    "                                if current_region[dy, dx] == current_max:\n",
    "                                    d_input[i, c, y*self.stride+dy, x*self.stride+dx] = d_out[i, c, y, x]\n",
    "        \n",
    "        return d_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationLayer:\n",
    "    def __init__(self, activation='relu'):\n",
    "        self.activation = activation\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.activation == 'relu':\n",
    "            self.last_input = x\n",
    "            return np.maximum(0, x)\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        elif self.activation == 'tanh':\n",
    "            return np.tanh(x)\n",
    "    \n",
    "    def backward(self, d_out):\n",
    "        if self.activation == 'relu':\n",
    "            d_input = d_out.copy()\n",
    "            d_input[self.last_input <= 0] = 0\n",
    "            return d_input\n",
    "        elif self.activation == 'sigmoid':\n",
    "            sig = self.forward(self.last_input)\n",
    "            return d_out * sig * (1 - sig)\n",
    "        elif self.activation == 'tanh':\n",
    "            tanh = self.forward(self.last_input)\n",
    "            return d_out * (1 - tanh**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    def __init__(self, input_size, output_size, learning_rate=0.01, lam_l1=0.01, lam_l2=0.01):\n",
    "        self.weights = np.random.randn(input_size, output_size) * 0.01\n",
    "        self.bias = np.random.randn(output_size, 1)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lam_l1 = lam_l1\n",
    "        self.lam_l2 = lam_l2\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.last_input = x\n",
    "        return np.dot(x, self.weights) + self.bias.T\n",
    "    \n",
    "    def backward(self, d_out):\n",
    "        d_weights = np.dot(self.last_input.T, d_out) + l1_regularization(self.weights, self.lam_l1) + l2_regularization(self.weights, self.lam_l2)\n",
    "        d_bias = np.sum(d_out, axis=0).reshape(-1, 1)\n",
    "        d_input = np.dot(d_out, self.weights.T)\n",
    "        \n",
    "        self.weights -= self.learning_rate * d_weights\n",
    "        self.bias -= self.learning_rate * d_bias\n",
    "\n",
    "        return d_input, d_weights, d_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y_true, y_pred):\n",
    "    return ((y_true - y_pred) ** 2).mean()\n",
    "\n",
    "def mse_loss_grad(y_true, y_pred):\n",
    "    return 2 * (y_pred - y_true) / y_true.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLULayer:\n",
    "    def forward(self, x):\n",
    "        self.last_input = x\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        d_input = d_out.copy()\n",
    "        d_input[self.last_input <= 0] = 0\n",
    "        return d_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exps = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exps / np.sum(exps, axis=1, keepdims=True)\n",
    "\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    n_samples = y_true.shape[0]\n",
    "    y_pred = np.clip(y_pred, 1e-12, 1 - 1e-12)\n",
    "    logp = - np.log(y_pred[np.arange(n_samples), np.argmax(y_true, axis=1)])\n",
    "    loss = np.sum(logp) / n_samples\n",
    "    return loss\n",
    "\n",
    "def cross_entropy_loss_grad(y_true, y_pred):\n",
    "    n_samples = y_true.shape[0]\n",
    "    grad = y_pred.copy()\n",
    "    grad[np.arange(n_samples), np.argmax(y_true, axis=1)] -= 1\n",
    "    grad = grad / n_samples\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet:\n",
    "    def __init__(self, learning_rate=0.001):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.conv1 = ConvLayer(input_channels=1, num_filters=32, kernel_size=3, stride=1, padding=1, learning_rate=learning_rate)\n",
    "        self.relu1 = ActivationLayer(activation='relu')\n",
    "        self.pool1 = PoolLayer(pool_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = ConvLayer(input_channels=32, num_filters=64, kernel_size=3, stride=1, padding=1, learning_rate=learning_rate)\n",
    "        self.relu2 = ActivationLayer(activation='relu')\n",
    "        self.pool2 = PoolLayer(pool_size=2, stride=2)\n",
    "\n",
    "        self.conv3 = ConvLayer(input_channels=64, num_filters=128, kernel_size=3, stride=1, padding=1, learning_rate=learning_rate)\n",
    "        self.relu3 = ActivationLayer(activation='relu')\n",
    "        self.pool3 = PoolLayer(pool_size=2, stride=2)\n",
    "\n",
    "        self.fc1 = DenseLayer(input_size=128 * 3 * 3, output_size=256, learning_rate=learning_rate)\n",
    "        self.relu4 = ActivationLayer(activation='relu')\n",
    "        \n",
    "        self.fc2 = DenseLayer(input_size=256, output_size=128, learning_rate=learning_rate)\n",
    "        self.relu5 = ActivationLayer(activation='relu')\n",
    "        \n",
    "        self.fc3 = DenseLayer(input_size=128, output_size=10, learning_rate=learning_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1.forward(x)\n",
    "        x = self.relu1.forward(x)\n",
    "        x = self.pool1.forward(x)\n",
    "        \n",
    "        x = self.conv2.forward(x)\n",
    "        x = self.relu2.forward(x)\n",
    "        x = self.pool2.forward(x)\n",
    "        \n",
    "        x = self.conv3.forward(x)\n",
    "        x = self.relu3.forward(x)\n",
    "        x = self.pool3.forward(x)\n",
    "        \n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = self.fc1.forward(x)\n",
    "        x = self.relu4.forward(x)\n",
    "        \n",
    "        x = self.fc2.forward(x)\n",
    "        x = self.relu5.forward(x)\n",
    "        \n",
    "        x = self.fc3.forward(x)\n",
    "        \n",
    "        return softmax(x)\n",
    "\n",
    "    def backward(self, d_out):\n",
    "        d_out = self.fc3.backward(d_out)[0]\n",
    "        d_out = self.relu5.backward(d_out)\n",
    "        d_out = self.fc2.backward(d_out)[0]\n",
    "        d_out = self.relu4.backward(d_out)\n",
    "        d_out = self.fc1.backward(d_out)[0]\n",
    "        \n",
    "        d_out = d_out.reshape(-1, 128, 3, 3)\n",
    "        \n",
    "        d_out = self.pool3.backward(d_out)\n",
    "        d_out = self.relu3.backward(d_out)\n",
    "        d_out = self.conv3.backward(d_out)[0]\n",
    "        \n",
    "        d_out = self.pool2.backward(d_out)\n",
    "        d_out = self.relu2.backward(d_out)\n",
    "        d_out = self.conv2.backward(d_out)[0]\n",
    "        \n",
    "        d_out = self.pool1.backward(d_out)\n",
    "        d_out = self.relu1.backward(d_out)\n",
    "        d_out = self.conv1.backward(d_out)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "y_train shape: (60000, 10)\n",
      "x_test shape: (10000, 28, 28, 1)\n",
      "y_test shape: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def load_mnist_images(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        f.read(16)  # 跳过前16个字节\n",
    "        images = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "        images = images.reshape(-1, 28, 28, 1)  # 28x28图像\n",
    "        return images\n",
    "\n",
    "def load_mnist_labels(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        f.read(8)  # 跳过前8个字节\n",
    "        labels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "        return labels\n",
    "\n",
    "# 加载数据\n",
    "x_train = load_mnist_images('data//MNIST//raw//train-images-idx3-ubyte')\n",
    "y_train = load_mnist_labels('data//MNIST//raw//train-labels-idx1-ubyte')\n",
    "x_test = load_mnist_images('data//MNIST//raw//t10k-images-idx3-ubyte')\n",
    "y_test = load_mnist_labels('data//MNIST//raw//t10k-labels-idx1-ubyte')\n",
    "\n",
    "# 预处理数据\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# 将标签转换为 one-hot 编码\n",
    "y_train = np.eye(10)[y_train]\n",
    "y_test = np.eye(10)[y_test]\n",
    "\n",
    "print(f'x_train shape: {x_train.shape}')\n",
    "print(f'y_train shape: {y_train.shape}')\n",
    "print(f'x_test shape: {x_test.shape}')\n",
    "print(f'y_test shape: {y_test.shape}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# 数据预处理\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# 加载 MNIST 训练集和测试集\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "\n",
    "# 选择一小部分数据进行训练和测试\n",
    "train_subset = Subset(trainset, list(range(1000)))  # 使用前1000个训练样本\n",
    "test_subset = Subset(testset, list(range(1000)))    # 使用前1000个测试样本\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(train_subset, batch_size=32, shuffle=True, num_workers=2)\n",
    "testloader = torch.utils.data.DataLoader(test_subset, batch_size=32, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 由于使用cpu训练，为了时间效率，仅使用前1000个样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 2.647\n",
      "Epoch 2/10, Loss: 2.568\n",
      "Epoch 3/10, Loss: 2.701\n",
      "Epoch 4/10, Loss: 2.613\n",
      "Epoch 5/10, Loss: 2.588\n",
      "Epoch 6/10, Loss: 2.615\n",
      "Epoch 7/10, Loss: 2.645\n",
      "Epoch 8/10, Loss: 2.528\n",
      "Epoch 9/10, Loss: 2.581\n",
      "Epoch 10/10, Loss: 2.379\n",
      "Finished Training\n",
      "Accuracy of the network on the 10000 test images: 84.00%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 假设 ConvLayer, ReLULayer, PoolLayer, DenseLayer 已经定义\n",
    "\n",
    "# 初始化模型\n",
    "model = AlexNet(learning_rate=0.001)\n",
    "\n",
    "# 训练参数\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "# 训练模型\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "\n",
    "        # 将数据转换为 numpy 数组\n",
    "        inputs = inputs.numpy()\n",
    "        labels = np.eye(10)[labels.numpy()]  # 转换为 one-hot 编码\n",
    "\n",
    "        # 前向传播\n",
    "        y_pred = model.forward(inputs)\n",
    "\n",
    "        # 计算损失和梯度\n",
    "        loss = cross_entropy_loss(labels, y_pred)\n",
    "        loss_grad = cross_entropy_loss_grad(labels, y_pred)\n",
    "\n",
    "        # 反向传播和更新参数\n",
    "        model.backward(loss_grad)\n",
    "\n",
    "        running_loss += loss\n",
    "\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Loss: {running_loss / len(trainloader):.3f}')\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "# 测试模型性能\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "for i, data in enumerate(testloader, 0):\n",
    "    inputs, labels = data\n",
    "\n",
    "    # 将数据转换为 numpy 数组\n",
    "    inputs = inputs.numpy()\n",
    "    labels = labels.numpy()\n",
    "\n",
    "    y_pred = model.forward(inputs)\n",
    "    correct_predictions += np.sum(np.argmax(y_pred, axis=1) == labels)\n",
    "    total_predictions += labels.shape[0]\n",
    "\n",
    "accuracy = correct_predictions / total_predictions\n",
    "print(f'Accuracy of the network on the 10000 test images: {accuracy * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
