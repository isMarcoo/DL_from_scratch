{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW:\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.W1 = np.random.randn(vocab_size, embedding_dim) * 0.01\n",
    "        self.W2 = np.random.randn(embedding_dim, vocab_size) * 0.01\n",
    "\n",
    "    def save_model(self, model_dir='./CBOW'):\n",
    "        os.makedirs(model_dir, exist_ok=True)  # 确保目录存在\n",
    "        np.save(os.path.join(model_dir, 'W1.npy'), self.W1)\n",
    "        np.save(os.path.join(model_dir, 'W2.npy'), self.W2)\n",
    "\n",
    "    def load_model(self, model_dir='./CBOW'):\n",
    "        os.makedirs(model_dir, exist_ok=True)  # 确保目录存在\n",
    "        self.W1 = np.load(os.path.join(model_dir, 'W1.npy'))\n",
    "        self.W2 = np.load(os.path.join(model_dir, 'W2.npy'))\n",
    "    \n",
    "    def cosine_similarity(self, vec1, vec2):\n",
    "        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "    def softmax(self, x):\n",
    "        e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return e_x / e_x.sum(axis=1, keepdims=True)\n",
    "\n",
    "    def forward_backward(self, contexts, targets, learning_rate):\n",
    "        # 前向传播\n",
    "        h = np.mean(self.W1[contexts], axis=1)  # 批量中每个上下文的平均词向量\n",
    "        u = np.dot(h, self.W2)\n",
    "        y_pred = self.softmax(u)\n",
    "\n",
    "        # 计算损失\n",
    "        losses = -np.log(y_pred[np.arange(len(targets)), targets])\n",
    "        loss = np.mean(losses)\n",
    "\n",
    "        # 反向传播\n",
    "        e = y_pred\n",
    "        e[np.arange(len(targets)), targets] -= 1\n",
    "        e /= len(targets)\n",
    "\n",
    "        dW2 = np.dot(h.T, e)\n",
    "        grad_input = np.dot(e, self.W2.T)  # 计算输入层梯度\n",
    "\n",
    "        # 初始化dW1为0\n",
    "        dW1 = np.zeros_like(self.W1)\n",
    "\n",
    "        # 更新每个上下文词的梯度\n",
    "        for i, context_words in enumerate(contexts):\n",
    "            for context_word in context_words:\n",
    "                dW1[context_word] += grad_input[i] / len(context_words)\n",
    "\n",
    "        # 更新权重\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.W1 -= learning_rate * dW1\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def train(self, data, epochs=50, learning_rate=0.1, batch_size=1024):\n",
    "        for epoch in range(epochs):\n",
    "            np.random.shuffle(data)\n",
    "            total_loss = 0\n",
    "            batches = [data[i:i + batch_size] for i in range(0, len(data), batch_size)]\n",
    "            for batch in batches:\n",
    "                contexts = np.array([x[0] for x in batch])\n",
    "                targets = np.array([x[1] for x in batch])\n",
    "                loss = self.forward_backward(contexts, targets, learning_rate)\n",
    "                total_loss += loss * len(batch)\n",
    "            print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "def build_vocabulary(tokens):\n",
    "    vocabulary = list(set(tokens))\n",
    "    word_to_index = {word: i for i, word in enumerate(vocabulary)}\n",
    "    return word_to_index, vocabulary\n",
    "\n",
    "def generate_training_data(tokens, word_to_index, window_size=2):\n",
    "    data = []\n",
    "    for i in range(window_size, len(tokens) - window_size):\n",
    "        context = [word_to_index[tokens[j]] for j in range(i - window_size, i + window_size + 1) if j != i]\n",
    "        target = word_to_index[tokens[i]]\n",
    "        data.append((context, target))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 8.163079134069969\n",
      "Epoch 2, Loss: 8.163075104779086\n",
      "Epoch 3, Loss: 8.163071071526051\n",
      "Epoch 4, Loss: 8.163067032220722\n",
      "Epoch 5, Loss: 8.163062977035752\n",
      "Epoch 6, Loss: 8.163058914991133\n",
      "Epoch 7, Loss: 8.163054835052758\n",
      "Epoch 8, Loss: 8.163050752248548\n",
      "Epoch 9, Loss: 8.163046648736186\n",
      "Epoch 10, Loss: 8.16304252892463\n",
      "Epoch 11, Loss: 8.163038387692149\n",
      "Epoch 12, Loss: 8.163034235151603\n",
      "Epoch 13, Loss: 8.163030053962355\n",
      "Epoch 14, Loss: 8.163025848956098\n",
      "Epoch 15, Loss: 8.16302162204616\n",
      "Epoch 16, Loss: 8.1630173728625\n",
      "Epoch 17, Loss: 8.163013092321693\n",
      "Epoch 18, Loss: 8.163008791585144\n",
      "Epoch 19, Loss: 8.163004469080485\n",
      "Epoch 20, Loss: 8.16300010865888\n",
      "Epoch 21, Loss: 8.162995714569677\n",
      "Epoch 22, Loss: 8.162991284146326\n",
      "Epoch 23, Loss: 8.162986820290511\n",
      "Epoch 24, Loss: 8.162982331470271\n",
      "Epoch 25, Loss: 8.162977790387632\n",
      "Epoch 26, Loss: 8.162973209266367\n",
      "Epoch 27, Loss: 8.162968586642773\n",
      "Epoch 28, Loss: 8.162963911328392\n",
      "Epoch 29, Loss: 8.162959195833283\n",
      "Epoch 30, Loss: 8.162954439992248\n",
      "Epoch 31, Loss: 8.162949627613152\n",
      "Epoch 32, Loss: 8.162944761961663\n",
      "Epoch 33, Loss: 8.162939849627127\n",
      "Epoch 34, Loss: 8.162934880975158\n",
      "Epoch 35, Loss: 8.162929852315106\n",
      "Epoch 36, Loss: 8.16292477001637\n",
      "Epoch 37, Loss: 8.162919628758354\n",
      "Epoch 38, Loss: 8.162914410370472\n",
      "Epoch 39, Loss: 8.162909141447084\n",
      "Epoch 40, Loss: 8.162903806363156\n",
      "Epoch 41, Loss: 8.162898402505464\n",
      "Epoch 42, Loss: 8.162892918851574\n",
      "Epoch 43, Loss: 8.16288737251717\n",
      "Epoch 44, Loss: 8.162881761216067\n",
      "Epoch 45, Loss: 8.162876056896922\n",
      "Epoch 46, Loss: 8.162870281071584\n",
      "Epoch 47, Loss: 8.162864415537115\n",
      "Epoch 48, Loss: 8.162858477348681\n",
      "Epoch 49, Loss: 8.16285244859483\n",
      "Epoch 50, Loss: 8.16284632683766\n"
     ]
    }
   ],
   "source": [
    "# 运行训练\n",
    "text = read_text_file('text8')\n",
    "part_text = text[:int(len(text) * 0.001)]  # 使用0.1%的数据\n",
    "tokens = tokenize_text(part_text)\n",
    "word_to_index, vocabulary = build_vocabulary(tokens)\n",
    "data = generate_training_data(tokens, word_to_index)\n",
    "\n",
    "# 初始化 CBOW 模型\n",
    "cbow = CBOW(len(vocabulary), embedding_dim=100)\n",
    "cbow.train(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
