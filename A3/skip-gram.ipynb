{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2Vec:\n",
    "    def __init__(self, vocab_size, embedding_dim=100, learning_rate=0.01):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.W1 = np.random.rand(vocab_size, embedding_dim)\n",
    "        self.W2 = np.random.rand(embedding_dim, vocab_size)\n",
    "\n",
    "    def save_model(self, model_dir='./word2vec'):\n",
    "        os.makedirs(model_dir, exist_ok=True)  # 确保目录存在\n",
    "        np.save(os.path.join(self.model_dir, 'W1.npy'), self.W1)\n",
    "        np.save(os.path.join(self.model_dir, 'W2.npy'), self.W2)\n",
    "\n",
    "    def load_model(self, model_dir='./word2vec'):\n",
    "        os.makedirs(model_dir, exist_ok=True)  # 确保目录存在\n",
    "        self.W1 = np.load(os.path.join(self.model_dir, 'W1.npy'))\n",
    "        self.W2 = np.load(os.path.join(self.model_dir, 'W2.npy'))\n",
    "    \n",
    "    def cosine_similarity(self, vec1, vec2):\n",
    "        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "    def softmax(self, x):\n",
    "        e_x = np.exp(x - np.max(x))\n",
    "        return e_x / e_x.sum(axis=0)\n",
    "\n",
    "    def forward_backward(self, x, target):\n",
    "        h = self.W1[x]\n",
    "        u = np.dot(self.W2.T, h)\n",
    "        y_pred = self.softmax(u)\n",
    "        loss = -np.log(y_pred[target])\n",
    "        e = y_pred\n",
    "        e[target] -= 1\n",
    "        dW2 = np.outer(h, e)\n",
    "        dW1 = np.dot(self.W2, e)\n",
    "        return loss, dW1, dW2\n",
    "\n",
    "    def train(self, X, Y, batch_size=1024, epochs=10):\n",
    "        n_samples = len(X)\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            for i in range(0, n_samples, batch_size):\n",
    "                batch_X = X[i:i+batch_size]\n",
    "                batch_Y = Y[i:i+batch_size]\n",
    "                for x, y in zip(batch_X, batch_Y):\n",
    "                    loss, dW1, dW2 = self.forward_backward(x, y)\n",
    "                    self.W1[x] -= self.learning_rate * dW1\n",
    "                    self.W2 -= self.learning_rate * dW2\n",
    "                    total_loss += loss\n",
    "            print(f\"Epoch {epoch + 1}, Loss: {total_loss / n_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据和词汇表的处理\n",
    "def tokenize_text(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "def build_vocabulary(tokens):\n",
    "    vocabulary = list(set(tokens))\n",
    "    word_to_index = {word: i for i, word in enumerate(vocabulary)}\n",
    "    index_to_word = {i: word for i, word in enumerate(vocabulary)}\n",
    "    return word_to_index, index_to_word, vocabulary\n",
    "\n",
    "def generate_training_data(tokens, word_to_index, window_size=2):\n",
    "    n_tokens = len(tokens)\n",
    "    X = []\n",
    "    Y = []\n",
    "    for i in range(n_tokens):\n",
    "        start = max(0, i - window_size)\n",
    "        end = min(n_tokens, i + window_size + 1)\n",
    "        for j in range(start, end):\n",
    "            if i != j:\n",
    "                X.append(word_to_index[tokens[i]])\n",
    "                Y.append(word_to_index[tokens[j]])\n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于text8数据集过大，训练时长太长，因此这里仅使用前百分之0.1来训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 6.255813514644019\n",
      "Epoch 2, Loss: 6.047366952688088\n",
      "Epoch 3, Loss: 5.969298869413322\n",
      "Epoch 4, Loss: 5.902210123415412\n",
      "Epoch 5, Loss: 5.839691276507373\n",
      "Epoch 6, Loss: 5.7793955552743626\n",
      "Epoch 7, Loss: 5.720165067127052\n",
      "Epoch 8, Loss: 5.661411844977673\n",
      "Epoch 9, Loss: 5.602876795553143\n",
      "Epoch 10, Loss: 5.544511668154516\n"
     ]
    }
   ],
   "source": [
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "# 运行训练\n",
    "text = read_text_file('text8')\n",
    "part_text = text[:int(len(text) * 0.001)]  # 使用0.1%的数据\n",
    "tokens = tokenize_text(part_text)\n",
    "word_to_index, index_to_word, vocabulary = build_vocabulary(tokens)\n",
    "X, Y = generate_training_data(tokens, word_to_index, window_size=4)\n",
    "\n",
    "model = Word2Vec(len(vocabulary))\n",
    "model.train(X, Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
