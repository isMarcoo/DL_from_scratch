{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class GloVe:\n",
    "    def __init__(self, vocab_size, embedding_dim=100, x_max=100, alpha=0.75):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.W = np.random.randn(vocab_size, embedding_dim) * 0.001\n",
    "        self.Wt = np.random.randn(vocab_size, embedding_dim) * 0.001\n",
    "        self.b = np.zeros(vocab_size)\n",
    "        self.bt = np.zeros(vocab_size)\n",
    "        self.x_max = x_max\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def save_model(self, model_dir='./glove'):\n",
    "        os.makedirs(model_dir, exist_ok=True)  # 确保目录存在\n",
    "        np.save(os.path.join(model_dir, 'W.npy'), self.W)\n",
    "        np.save(os.path.join(model_dir, 'Wt.npy'), self.Wt)\n",
    "        np.save(os.path.join(model_dir, 'b.npy'), self.b)\n",
    "        np.save(os.path.join(model_dir, 'bt.npy'), self.bt)\n",
    "\n",
    "    def load_model(self, model_dir='./glove'):\n",
    "        os.makedirs(model_dir, exist_ok=True)  # 确保目录存在\n",
    "        self.W = np.load(os.path.join(model_dir, 'W.npy'))\n",
    "        self.Wt = np.load(os.path.join(model_dir, 'Wt.npy'))\n",
    "        self.b = np.load(os.path.join(model_dir, 'b.npy'))\n",
    "        self.bt = np.load(os.path.join(model_dir, 'bt.npy'))\n",
    "\n",
    "    def cosine_similarity(self, vec1, vec2):\n",
    "        \"\"\"计算两个向量之间的余弦相似度\"\"\"\n",
    "        return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "\n",
    "    def weight_fn(self, x):\n",
    "        return (x / self.x_max) ** self.alpha if x < self.x_max else 1\n",
    "\n",
    "    def train(self, cooccurrences, epochs=20, learning_rate=0.02):\n",
    "        for epoch in range(epochs):\n",
    "            shuffle(cooccurrences)\n",
    "            total_loss = 0\n",
    "            for i, j, Xij in cooccurrences:\n",
    "                weight = self.weight_fn(Xij)\n",
    "                dot = np.dot(self.W[i], self.Wt[j])\n",
    "                cost = (dot + self.b[i] + self.bt[j] - np.log(Xij)) ** 2\n",
    "                total_loss += weight * cost\n",
    "\n",
    "                grad_w = 2 * weight * (dot + self.b[i] + self.bt[j] - np.log(Xij))\n",
    "                grad_b = 2 * weight * (1 + self.b[i] + self.bt[j] - np.log(Xij))\n",
    "\n",
    "                self.W[i] -= learning_rate * grad_w * self.Wt[j]\n",
    "                self.Wt[j] -= learning_rate * grad_w * self.W[i]\n",
    "                self.b[i] -= learning_rate * grad_b\n",
    "                self.bt[j] -= learning_rate * grad_b\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}, Loss: {total_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "def tokenize_text(text):\n",
    "    # 简单文本预处理和分词\n",
    "    return text.lower().split()\n",
    "\n",
    "def build_vocabulary(tokens):\n",
    "    vocabulary = list(set(tokens))\n",
    "    word_to_index = {word: i for i, word in enumerate(vocabulary)}\n",
    "    index_to_word = {i: word for i, word in enumerate(vocabulary)}\n",
    "    return word_to_index, index_to_word, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import dok_matrix\n",
    "\n",
    "def build_cooccurrence_matrix(tokens, word_to_index, window_size=5):\n",
    "    vocab_size = len(word_to_index)\n",
    "    cooccurrences = dok_matrix((vocab_size, vocab_size), dtype=np.float32)\n",
    "\n",
    "    for i in range(len(tokens)):\n",
    "        token = tokens[i]\n",
    "        left = tokens[max(i - window_size, 0): i]\n",
    "        right = tokens[i + 1: i + 1 + window_size]\n",
    "\n",
    "        for distance, context_word in enumerate(left + right, 1):\n",
    "            j = i - window_size + distance if distance <= len(left) else i + distance - len(left)\n",
    "            weight = 1.0 / distance  # This can be tweaked to different weighting functions\n",
    "\n",
    "            cooccurrences[word_to_index[token], word_to_index[context_word]] += weight\n",
    "            cooccurrences[word_to_index[context_word], word_to_index[token]] += weight\n",
    "\n",
    "    return cooccurrences\n",
    "\n",
    "# 使用部分text8数据\n",
    "text = read_text_file('text8')\n",
    "part_text = text[:int(len(text) * 0.001)]  # 使用0.1%的数据\n",
    "tokens = tokenize_text(part_text)\n",
    "word_to_index, index_to_word, vocabulary = build_vocabulary(tokens)\n",
    "cooccurrences = build_cooccurrence_matrix(tokens, word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练时不知什么原因导致loss以3000+起始，且经过超参数的调整未能有有效的变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 3334.226509575537\n",
      "Epoch 2, Loss: 3428.4098207538545\n",
      "Epoch 3, Loss: 3551.5037310203766\n",
      "Epoch 4, Loss: 3563.3806471501143\n",
      "Epoch 5, Loss: 3466.7877195014566\n",
      "Epoch 6, Loss: 2996.375810389824\n",
      "Epoch 7, Loss: 2192.397449017376\n",
      "Epoch 8, Loss: 1752.127645821808\n",
      "Epoch 9, Loss: 1492.505948053505\n",
      "Epoch 10, Loss: 1371.9985983824379\n",
      "Epoch 11, Loss: 1286.6481533338797\n",
      "Epoch 12, Loss: 1243.694423216382\n",
      "Epoch 13, Loss: 1215.1792167542262\n",
      "Epoch 14, Loss: 1196.5045275870577\n",
      "Epoch 15, Loss: 1186.574153588268\n",
      "Epoch 16, Loss: 1181.3925086713764\n",
      "Epoch 17, Loss: 1178.5998664903036\n",
      "Epoch 18, Loss: 1179.3843938870036\n",
      "Epoch 19, Loss: 1178.603465395194\n",
      "Epoch 20, Loss: 1185.216029545383\n"
     ]
    }
   ],
   "source": [
    "def extract_cooccurrences(cooccurrences):\n",
    "    # 提取非零共现记录到列表\n",
    "    data = [(i, j, cooccurrences[i, j]) for i, j in cooccurrences.keys() if cooccurrences[i, j] > 0]\n",
    "    return data\n",
    "\n",
    "# 使用提取函数\n",
    "data = extract_cooccurrences(cooccurrences)\n",
    "\n",
    "# 初始化并训练模型\n",
    "glove = GloVe(len(vocabulary))\n",
    "glove.train(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
